<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Blog Sistemas Recomendadores</title>
    <link rel="stylesheet" href="index.css">
</head>
<body>
<div class="header">
    <h1>Sistemas Recomendadores - IIC3633</h1>
    <h2>Blog de Papers</h2>
    <h2>Sebasti√°n Montoya</h2>
</div>
<ul>
    <li>
        <h1>Deep Neural Networks for YouTube Recommendations</h1>
        <h4>Paul Covington, Jay Adams and Emre Sargin</h4>
        <p style="width: 50%">
            In this paper the authors explained their propose Deep Neural Network for making a candidate recommendation
            and how to rank them, in particular the videos from YouTube. First they talk about the candidate recommendation,
            and they explained it as a classification, an extreme multiclass. They used a softmax classifier but they weight
            the candidates so they have an order, instead of using a hierarchical softmax or other techniques applied in
            decision trees. Afterwards they talk about the architecture of the neural network, that for me is a perfect
            piece of engineering, how they are able to make the different information they have available per user to fit
            in the network and how to interpret the different inputs so they network can improve faster.
        </p>
        <p style="width: 50%">
            After reading lots of paper I definitely found this one the best, how they start from a very simple model but
            later they are able to incorporate different techniques and even avoid the negative sampling. And the idea
            of predicting the time the user will watch a video is simply incredible. Nevertheless, after reading the paper
            I asked myself how the model could be even better, probably by incorporating more metadata from the user, right
            now for example google chrome allows you to log in and it remains logged in for all the site which have a google
            log in system, it would be great if the network could include search history among other things. Maybe for music
            which songs have they listen to in Spotify and for a particular artist show in YouTube the videos of songs that
            Spotify does not have.
        </p>
    </li>
    <li>
        <h1>BPR: Bayesian Personalized Ranking from Implicit Feedback</h1>
        <h4>Steffen Rendle, Christoph Freudenthaler, Zeno Gantner and Lars Schmidt-Thieme</h4>
        <p style="width: 50%;">
            In this paper the authors present a new optimization criterion called BPR-OPT for personalized ranking from
            implicit feedback, and with this they also present a learning method called LearnBPR in order to maximize the
            usage of model that use BPR-OPT. Afterwards they talk about the difference between the model they are proposing
            and the state of art in this matter, where they make clear the fact that the model uses triplets of
            (user, item_1, item_2) instead of the classic models where it is used (user, item). In the next sections of
            the paper the authors make a more formal explanation of the optimization criterion and try to make the analogy
            with AUC. Finally they talk about LearnBPR, which is basically a stochastic gradient descent but the big difference
            is that it has to be complemented with bootstrap sampling. After the explanation of LearnBPR they talk about the
            models they are going to compare the new criterion they are proposing and how this criterion can be applied to
            classic recommender models as kNN and MF. In the last section they talk about the evaluation methodology and how
            this criterion makes the models increase their performance.
        </p>
        <p style="width: 50%;">
            Personally I found really interesting this paper, they are offering a new method to increase the perfomance of
            models that have been around for a lot of time in this subject. Particularly after reading the paper I found
            really interesting the fact of using a bootstrap sampling, which is something really common when making random
            forests ensemble models, and how they are able to apply this method that in other areas works great and discovering
            that in this task of recommeding items it has a similar effect on increasing the perfomance. In second place,
            I found interesting the fact that they are not trying to predict a number (ranking) of an item, instead they
            are trying to classify the difference between an item and another for a particular user. And in some way they
            change the typical way of understanding datasets for this models, as if a user haven't seen and object then it means
            that the user does not like that user. I found this paper simply excellent, very clear on their ideas but in
            some way a little challenging in the mathematical part and in trying to understand the many models that are
            being presented along the paper.
        </p>
    </li>
    <li>
        <h1>Content-Based Artwork Recommendation</h1>
        <h4>Pablo Messina, Vicente Rodriguez, Denis Parra, Cristoph Trattner and Alvaro Soto</h4>
        <p style="width: 50%">
            This paper talks about a recommender system that gives recommendations to it's users based on the content
            of the image and it's metadata. What I was able to understand they present different metrics in order to
            evaluate this particular recommender system and how to manage the content (pixels) of an image, and how
            to recommend similar items depending on this. Mainly the first part of the papet talks about the state of
            art in this area and it previous works, in the second part is mainly focused on defining different metrics
            that are the classical ones but adapted to this type of data of vectors representing the values of the pixels.
        </p>
        <p style="width: 50%">
            I really liked this paper, basically by three things in first place they are able to innovate and adapt
            the classic metrics to this particular area and second is how they make experiments on every metric and
            how making an ensemble of different metrics make the recommender system have an improvement in performance.
            Finally I found really good for the perspective of the reader how they compare the fact of using only the metadata
            , then the visual and metadata  afterwards an hybrid and finally comparing to an expert making an analysis.
            And how a Deep Neural Network is able to be much more accurate than a human and to find patterns in dimensions
            that we are not able to see.
        </p>
    </li>
    <li>
        <h1>Content-Based Recommendations Systems</h1>
        <h4>Michael J. Pazzani and Daniel Billsus</h4>
        <p style="width: 50%">
            After reading this paper I was left with a really good feeling, mainly because it was very simple and clear
            in delivering the right information so anyone can understand how learning algorithms can be changed and
            used for a new tasks. In this case they go algorithm by algorithm explaining how it works but most important
            they are able to explain which are its benefits and weaknesses. So in this way after reading the whole
            paper you are able to imagine how an ensemble of this different algorithms could improve your recommender
            system by taking the best part of each algorithm.
        </p>
        <p style="width: 50%">
            Last semester I had the course of AI and one assignment was about being able to predict a word with a given
            context of words. Because of this I found really interesting the fact of content based recommender system
            because as it is mentioned in the paper is really difficult to obtain the correct meaning on text free inputs.
            So NLP could be a really good improvement in this area and how could we be able to eliminate words that do
            not give any benefit or even how we give a meaning to the information depending on our culture and how we
            interpret the language. As I have already mentioned I found really interesting this type of recommender system,
            but further how to manage non structured data as free input texts.
        </p>
        <p style="width: 50%">
            Personally I have no negative comments about this paper as I already mentioned I founded pretty neat , precise
            adn clear on delivering the correct information to the reader.
        </p>
    </li>
    <li>
        <h1>Performance of Recommender algorithms on Top-N Recommendations tasks</h1>
        <h4>Paolo Cremonesi, Yehuda Koren and Roberto Turrin</h4>
        <p style="width: 50%"> This paper introduces the fact that the top-N recommendations should be evaluated over precision and recall
            instead of using the more classical metricas like RMSE or MAE. Mainly because they state the fact that
            a recommender system has good results in RMSE or MAE does not mean that it performs well for the top-N
            recommendations. So they propose the following, first of all eliminate the most popular or rated items
            so the systems are evaluated over the non-trivial recommendations, afterwards they use the train set for
            the Netflix dataset and for the test set they filter the Netflix test set only with the 5-star rated items.
            Then they take every item and predict the rating of it and 1000 items unrated by the user. After they define
            a hit or a succes if the position of the predicted item is in the first N position. Finally they explain
            every algorithm they are going to test and the final results where PureSVD performs the best in top-N
            recommendations.</p>
        <p style="width: 50%">
            I found this paper really interesting first because they are able to demonstrate that the most classical
            metrics are not always the best ones, which for me is a really strong fact. With that they try to look
            for a metric that was able to have good recommendations on the long tail head instead of the trivial
            recommendations, which is what a recommender system should target so it does not recommend the most popular
            or trivial items. Something I did not like in a visual perspective is that the analysis of the different
            charts is made on a totally different page so it is very hard to follow the explanation and afterwards looking
            to the charts. Something very particular is that they obtained a result they did not expected and even with that
            they are able to explain the results obtain. For me it is a really good and complete paper that explains every
            aspect of it.
        </p>
    </li>
    <li>
        <h1>Scalable Collaborative Filtering Approaches for Large Recommender Systems</h1>
        <h4>G√°bor Tak√°cs, Istv√°n Pil√°szy, Botty√°n N√©meth and Domonkos Tikk</h4>
        <p style="width: 50%">
            This paper is about matrix factorization and different variations of this technique, along the paper
            the authors start explaining every variation and how it was different from the others and which problems
            did the variation resolve. Also in the beginning of the paper the authors show how they consider a correct
            classification for MF algorithms and from that they start explaining each algorithm.
        </p>
        <p style="width: 50%">
            Personally I did not liked this paper mainly because I did not found it interesting, for me it was a paper
            focused mainly in a mathematical part of algorithms. Therefore I really liked the fact that the authors
            made tests for every different algorithms they presented before, and compared them in various test and
            metric's comparisons. But generally was a hard reading paper, I had to do a lot of research because the authors
            present technical terms or algorithms, because the authors probably make the assumption that someone with a
            really good mathematical background is the one who is reading it. Therefore I do not feel they are able to
            explain the algorithms in a very simple way, in order that any person could get interested in this area by
            simply reading this paper.
        </p>
    </li>
    <li>
        <h1>Collaborative Filtering for Implicit Feedback Datasets</h1>
        <h4>Yifan Hu, Yehuda Koren and Chris Volinsky</h4>
        <p style="width: 50%">
            The authors of this paper, present a new algorithm for implicit feedback. As it is expected the authors
            first explain what is implicit feedback, that refers to all the actions that a user makes but without asking
            or receiving an explicit input. For example asking a survey if you liked or not a certain tv show, in the
            other hand implicit feedback could be the amount of time spent watching a specific tv show. For this they
            present a new algorithm that is based on the fact, that implicit feedback does not receive any negative inputs
            for example they can't know if a user disliked a program therefore they are able to suppose that if a user
            watch for a little time a tv show is probably because they did not like it. So they decide that instead of
            using a prediction of the rating, they are going to use a certain preference and a confidence on that amount
            of preference. So after some algebraic and mathematical optimizations they are able to propose a model that
            is able to predict a list of items for a user based on their activities with tv shows.
        </p>
        <p style="width: 50%">
            I really liked this paper because it explains how the implicit feedback works and they describe the main
            problems this type of feedback presents. The authors after introducing the main idea of the implicit feedback
            start to describe the multiple problems you can bump with and how to found or implement a solution for it.
            I found really interesting how they are able to create sort of new metrics based on the idea of preference and
            confidence. Also after reading the whole paper I notice that lots of the assumptions where made in the model,
            particularly the part where they set to zero all values less than 0.5, personally I found interesting analyzing
            this data and search a meaning for them because sometimes this type of outliers could contribute to design a
            better recommender system.
        </p>
    </li>
    <li>
        <h1>Slope One Predictors for Online Rating-Based Collaborative Filtering</h1>
        <h4>Daniel Lemire and Anna Maclachlan</h4>
        <p style="width: 50%">
            Mainly this paper talks about this new algorithm that is being proposed the Slope One, in the paper the
            authors get to explain the purposes a Collaborative Scheme should accomplish and how they start building
            this algorithms and which are their baseline for it. Further in the paper they get to explain the different
            types or variations that could be applied.
        </p>
        <p style="width: 50%">
            In my personal opinion I founded this paper really precise on what they are trying to present, so in this
            way the paper explains pretty concisely how the algorithm works and it variation. Also they include the
            variations in order for the reader to figure out which one is better and with that a comparison chart in
            work.
        </p>
        <p style="width: 50%">
            Something that I didn't like to much is the fact of the high level of mathematics being used in the paper
            definetely it limits the readers that could be able to understand the algorithm they are proposing. I think
            it would be interesting if in some way they could be able at first make a common reader understand how the
            algorithm works and afterwards introduce the hard mathematical part.
        </p>
    </li>
    <li>
        <h1>Collaborative Filtering Recommender Systems </h1>
        <h4>J. Ben Schafer, Dan Frankowski, Jon Herlocker and Shilad Sen</h4>
        <p style="width: 50%">
            This paper talks about how do collaborative filtering recommender systems work, in particular it starts
            telling the reader some examples of this type of recommender systems so the reader can start to get involve
            and recognize of what type of systems they are talking. Further in the paper it talks about
            how the users get involved with the systems and which are there duties when it comes to collaborative
            filtering. Afterwards it talks about the purpose or tasks that recommender systems try to acheive, for this
            it starts explaining the different types of algorithms depending on user or items and how they can be
            improved. For every algorithm also states the various practical problems that each one of them have.
            Finally it presents for the reader the different ways that you can evaluate a recommender system of this
            class, and afterwards the major tradeoffs you need to deal when developing this systems.
        </p>
        <p style="width: 50%">
            Personally I really liked how to the paper is structured because at first you get to understand the various
            algorithms and how do they works, so afterwards you are able to understand in a better way their limitations
            and because of that how important is to make a mixture of different recommender systems. Also something I
            really enjoyed is the part where it talks about how much information are the people willing to give in order
            to receive the best predictions, mostly because this is something people are talking about a lot given the
            global environment on this issue. I found really interesting that they have a section where they propose open
            questions so the reader is able to think and even question some things the authors propose.
        </p>
        <p style="width: 50%">
            As I have already stated I really like this paper therefore I founded challenging in some parts, and because
            of that I had to lookup for extra information when reading it. I also think that I would like more information
            on novelty and how it can improve your recommender system. But in general is a paper I really enjoyed and learned
            a lot from it.
        </p>
    </li>
</ul>
</body>
</html>